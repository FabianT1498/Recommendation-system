{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIBRARIES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import itertools as it\n",
    "from operator import itemgetter\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import sys\n",
    "import random\n",
    "sys.path.append('../src')\n",
    "\n",
    "from _nmf_ACCSLP import NMF_ACCSLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS \n",
    "WEIGHT_THRESHOLD = 1\n",
    "TEST_SET_RATIO = 0.4\n",
    "\n",
    "K = 3\n",
    "range_K = [3,5,10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.options.display.width = 0\n",
    "pd.options.mode.chained_assignment = None\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_community_information_matrix(cluster_assignments):\n",
    "    \n",
    "    # Crear la matriz de membresía\n",
    "    num_nodes = len(cluster_assignments)\n",
    "\n",
    "    membership_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for i in range(num_nodes):\n",
    "        for j in range(num_nodes):\n",
    "            # Verificar si los elementos pertenecen al mismo cluster\n",
    "            if cluster_assignments[i] == cluster_assignments[j]:\n",
    "                membership_matrix[i, j] = 1\n",
    "    return membership_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying k-means algorithm \n",
    "def apply_kmeans(features, n_clusters, kmeans_kwargs, x, y, feature_names):\n",
    "\n",
    "    k = n_clusters\n",
    "    kmeans = KMeans(n_clusters=k, **kmeans_kwargs)\n",
    "    kmeans.fit(features)\n",
    "    kmeans_df =  pd.DataFrame(\n",
    "        features,\n",
    "        columns=feature_names\n",
    "    )\n",
    "\n",
    "    kmeans_df[\"predicted_cluster\"] = kmeans.labels_\n",
    "\n",
    "    return kmeans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rbf_to_get_similarity(features, gamma=1):\n",
    "    rbf_matrix = rbf_kernel(features, gamma=gamma)  # Ajusta gamma según tu necesidad\n",
    "    return rbf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareNodes(f_list, s_list):\n",
    "    \"\"\"\n",
    "        Function that returns the number of users that have interact with both items\n",
    "        Funcion que devuelve el numero de usuarios que han interactuado con ambos items\n",
    "    \"\"\"\n",
    "    weight = len(np.intersect1d(f_list, s_list))\n",
    "    \n",
    "    return weight\n",
    "    \n",
    "def createLinks(prob_us_set, nodes, threshold):\n",
    "    \"\"\"\n",
    "        Function that creates graph links with the information about the set. The weight has to be grater or equal to threshold.\n",
    "        \n",
    "        Funcion que crea los enlaces del grafo a partir de la informacion contenida en el conjunto que se le\n",
    "        pasa a la funcion. El peso tiene que ser mayor o igual al umbral.\n",
    "        \n",
    "        Format of links list -> [(Node1, Node2, weight), ......]\n",
    "    \"\"\"\n",
    "    result = list()  \n",
    "    \n",
    "    # hago todas las posibles combinaciones de problemas\n",
    "    for fst, snd in it.combinations(nodes, 2):\n",
    "        # obtengo el peso pasando la lista de usuarios que ha hecho cada problema\n",
    "        weight = compareNodes(prob_us_set[fst], prob_us_set[snd])\n",
    "\n",
    "        # If at least two users have been interacted with the same two items, then add link to list, else, omit the interactions.\n",
    "        if weight >= threshold:\n",
    "            result.append((fst, snd))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_nx(list_nodes, list_links):\n",
    "    \"\"\"\n",
    "        Function that creates a graph with the format from NetworkX \n",
    "        \n",
    "        Funcion que crea un grafo de tipo Graph de la libreria NetworkX\n",
    "        Construccion del grafo: http://networkx.readthedocs.io/en/networkx-1.11/tutorial/tutorial.html#what-to-use-as-nodes-and-edges\n",
    "    \"\"\"\n",
    "    grafo = nx.Graph() # creo la variable grafo\n",
    "\n",
    "    # incluyo los nodos del grafo \n",
    "    grafo.add_nodes_from(list_nodes)\n",
    "\n",
    "    # se incluyen las tuplas de enlaces con el peso del enlace\n",
    "    # es una lista de la forma [(Nodo1, Nodo2, peso), ......]\n",
    "    grafo.add_edges_from(list_links)\n",
    "\n",
    "    return grafo\n",
    "\n",
    "def build_adjacency_matrix(graph, list_nodes):\n",
    "    return nx.to_numpy_array(graph, nodelist=list_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKrecommendations(index, items_eval, item_index, items, S_prime):\n",
    "    # list_recom = dict()\n",
    "    # offset = item_index\n",
    "    # total_columns = S_prime.shape[1]\n",
    "    \n",
    "    # for j in range(offset, total_columns):\n",
    "    #     if j != offset and S_prime[index][j] > 0.00 and items[j] not in items_eval:\n",
    "    #         list_recom[items[j]] = S_prime[index][j]\n",
    "\n",
    "    list_recom = dict()\n",
    "    total_columns = S_prime.shape[1]\n",
    "    \n",
    "    for j in range(total_columns):\n",
    "        if j != item_index and S_prime[index][j] > 0.00 and items[j] not in items_eval:\n",
    "            list_recom[items[j]] = S_prime[index][j]\n",
    " \n",
    "    return list(list_recom.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeighing(item, items_recom_with_values):\n",
    "    weight = sum([value for (it, value) in items_recom_with_values if it == item])\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delRepetitions(lista):\n",
    "    \"\"\"\n",
    "        Funcion auxiliar para evitar que salgan repeticiones en las recomendaciones. Saco la lista de posibles \n",
    "        recomendaciones con valores unicos\n",
    "    \"\"\"\n",
    "    conjunto_vacio = set()\n",
    "    \n",
    "    # esto sirve para que se haga mas rapido la comprobacion de si el elemento esta en la lista o no\n",
    "    function_add = conjunto_vacio.add\n",
    "    \n",
    "    # hago la lista intensional, para mantener el orden dado en la lista original\n",
    "    return [x for x in lista if not (x in conjunto_vacio or function_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListRecom(items_train, items_train_indexes, S_prime, k, items, items_eval):\n",
    "    list_recommendations = list()\n",
    "\n",
    "    for idx, item in enumerate(items_train):\n",
    "        # Esto va a contener una lista de tuplas del tipo (item-recomendar, valor similitud), donde el item puede aparecer varias veces repetido\n",
    "        list_recommendations = list_recommendations + getKrecommendations(idx, items_train, items_train_indexes[idx], items, S_prime)\n",
    "\n",
    "    # Contiene la suma de todos los valroes de similitud\n",
    "    total = sum([x for (_,x) in list_recommendations])\n",
    "\n",
    "    # Esto va a contener solo los items a recomendar sin repetición y sin valores\n",
    "    items_recom_no_values = list(set([item for (item, _) in list_recommendations]))\n",
    "    \n",
    "    # Sistema de votación ponderada: para cada item que aparezca, sumar todos sus valores de similitud asociado / total\n",
    "    items_recom = [(item, getWeighing(item, list_recommendations)/total) for item in items_recom_no_values]\n",
    "    \n",
    "    items_recom.sort(key=itemgetter(1), reverse=True)\n",
    "\n",
    "    print (\"Items a recomendar, con su peso: \", items_recom)\n",
    "    \n",
    "    # y me quedo con el primer elemento de la tupla, que es el item a recomendar\n",
    "    list_sim_final = [x for (x,_) in items_recom] \n",
    "        \n",
    "    # ahora elimino los items que estan en la lista de items con los que ha interactuado el target user\n",
    "    list_final = [x for x in list_sim_final if x not in items_train]\n",
    "    \n",
    "    # y quito las repeticiones\n",
    "    list_final = delRepetitions(list_final)\n",
    "\n",
    "    print (\"list_final\", list_final)\n",
    "\n",
    "    # list_fin_rec = [x for x in list_final if x in items_eval] # Esta linea de codigo es solo para filtrar cuales items ocultos han sido predecidos\n",
    "\n",
    "    return list_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_getKrecommendations(df_new, items_train, items_train_indexes, S_prime, k, items, items_eval):\n",
    "    \"\"\"\n",
    "    Function to generate a new column with the list of recommendations for each user\n",
    "    \"\"\"\n",
    "    df_new['list_recommendations_original'] = df_new.apply(lambda row: getListRecom(items_train, items_train_indexes, S_prime, k, items, items_eval), axis=1)\n",
    "    \n",
    "    # df_new['list_recommendations'] = (df_new.copy()).apply(lambda row: row['list_recommendations_original'][:k], axis=1)\n",
    "\n",
    "    df_new['list_recommendations'] = [row['list_recommendations_original'][:k] for _, row in df_new.copy().iterrows()]\n",
    "\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_prediction_matrix(S_prime, items_eval_indexes):\n",
    "    return np.take(S_prime, items_eval_indexes, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMetricsResults(list_recom_items, list_recom_items_original, user_list_to_recommend, list_train_items, list_eval_items, k):    \n",
    "    set_df_metric = {'user_id': user_list_to_recommend, 'train_items': [list_train_items], 'eval_items': [list_eval_items], 'recom_items': list_recom_items, 'recom_items_original': list_recom_items_original}\n",
    "    metric_df = pd.DataFrame.from_dict(set_df_metric)\n",
    "    print (metric_df)\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_file(dir, result, k):\n",
    "    f = open(dir, 'a')\n",
    "    f.write('K, one_hit, precision, mrr, recall, f1 \\n')\n",
    "    f.write(str(k) + ',' + str(result['one_hit']) + ',' + str(result['precision']) + ',' + str(result['mrr']) + ',' + str(result['recall']) + ',' +  str(result['f1']) + '\\n') \n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hit(row):\n",
    "    \"\"\"\n",
    "        Funcion que implementa la metrica one hit. Devuelve un 1 si para un usuario dado, ese usuario ha interactuado \n",
    "        con al menos uno de los items que se le ha recomendado en el evaluation_set. \n",
    "        Cero si no hay ningun item de los recomendados con los que el usuario haya interactuado\n",
    "    \"\"\"\n",
    "    num_items_common = np.intersect1d(row['recom_items'], row['eval_items'])\n",
    "    \n",
    "    if len(num_items_common) >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(row): \n",
    "    \"\"\"\n",
    "        Funcion que va a implementar la metrica de evaluacion mrr:\n",
    "        mrr = 1/ranki, donde ranki es la posicion del primer item correcto\n",
    "    \"\"\"\n",
    "\n",
    "    num_items_common = np.intersect1d(row['recom_items'], row['eval_items'])\n",
    "    \n",
    "    if len(num_items_common) >= 1:\n",
    "\n",
    "        # hago la busqueda del primer elemento que esta en la lista de recomendados\n",
    "        fst_correct_item = -1\n",
    "        encontrado = False\n",
    "        i = 0\n",
    "        ranki = 0\n",
    "        #print(ranki)\n",
    "        while (i < len(row['recom_items'])) and (encontrado == False):\n",
    "            if row['recom_items'][i] in row['eval_items']:\n",
    "                # fst_correct_item = row['recom_items'][i]\n",
    "                # print(fst_correct_item)\n",
    "                ranki = i + 1\n",
    "                encontrado = True\n",
    "                #print(\"entro\")\n",
    "            else:\n",
    "                i = i + 1\n",
    "                \n",
    "        if ranki == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return (1/ranki)\n",
    "\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(row):\n",
    "    \"\"\"\n",
    "        Funcion que va a implementar la metrica precision en k: \n",
    "        (cuantos de los interactuados con el usuario estan entre los recomendados) / todos los recomendados\n",
    "    \"\"\"\n",
    "    \n",
    "    num_items_common = np.intersect1d(row['recom_items'], row['eval_items'])\n",
    "    \n",
    "    if len(row['recom_items']) == 0:\n",
    "        return float(0)\n",
    "\n",
    "    return float(len(num_items_common)/len(row['recom_items']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(row):\n",
    "    \"\"\"\n",
    "        Funcion que implementa la metrica recall\n",
    "        (cuantos de los interactuados con el usuario estan entre los recomendados) / todos los evaluados\n",
    "    \"\"\"  \n",
    "    num_items_common = np.intersect1d(row['recom_items'], row['eval_items'])\n",
    "\n",
    "    if (len(row['eval_items']) == 0):\n",
    "        return 0\n",
    "     \n",
    "    return (len(num_items_common)/len(row['eval_items']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(row):\n",
    "    \"\"\"\n",
    "        Funcion que calcula el f1 en funcion de precision y recall\n",
    "    \"\"\"\n",
    "    denominador = row['precision'] + row['recall']\n",
    "    \n",
    "    if denominador == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (2 * row['precision'] * row['recall']) / denominador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateScoreResults(metric_df):\n",
    "    \"\"\"\n",
    "        Function to build a dataframe with the results for the evaluation metrics\n",
    "    \"\"\"\n",
    "    metric_df['one_hit'] = metric_df.apply(lambda row: one_hit(row), axis=1)\n",
    "    metric_df['mrr'] = metric_df.apply(lambda row: mrr(row), axis=1)\n",
    "    metric_df['precision'] = metric_df.apply(lambda row: precision(row), axis=1)\n",
    "    metric_df['recall'] = metric_df.apply(lambda row: recall(row), axis=1)\n",
    "    metric_df['f1'] = metric_df.apply(lambda row: f1(row), axis=1)\n",
    "   \n",
    "    result_one_hit = metric_df['one_hit'].mean()\n",
    "    result_precision = metric_df['precision'].mean()\n",
    "    result_mrr = metric_df['mrr'].mean()\n",
    "    result_recall = metric_df['recall'].mean()\n",
    "    result_f1 = metric_df['f1'].mean()\n",
    "   \n",
    "    # voy a crear un diccionario con los resultados\n",
    "    results_metrics = {'one_hit': result_one_hit, 'precision': result_precision, 'mrr': result_mrr, 'recall': result_recall, 'f1': result_f1}\n",
    "    \n",
    "    return results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    items: List of all itemds\n",
    "    user_items_train: items that user have interacted\n",
    "    user_items_train_indexes: indexes of items that user have interacted\n",
    "    df_interactions: Dataframe of all interactions\n",
    "    user_items_eval: items that user have interacted and have been hidden from training set\n",
    "    user: user id\n",
    "\"\"\"\n",
    "def main_process(items, user_items_train, user_items_train_indexes, df_interactions, user_items_eval, user):\n",
    "    \n",
    "    # diccionario que va a contener como key el user, como value, los items con los que ha interactuado el user\n",
    "    df_users_simple = {}\n",
    "    grouped_user = df_interactions.groupby('user')['item'].apply(list)\n",
    "    for i,j in zip(grouped_user.index.tolist(), grouped_user.values.tolist()):\n",
    "        df_users_simple[i] = j \n",
    "\n",
    "    # PARA LA CONSTRUCCION DE R PRECISION -----------\n",
    "   \n",
    "     # convierto la serie en un dataframe\n",
    "    # df_user_eval = pd.DataFrame({'user_id': [user], 'list_item_id': [user_items_eval]})\n",
    "\n",
    "    # df_user_eval[\"items_watched\"] = [df_users_simple[row['user_id']] for _, row in df_user_eval.iterrows()]\n",
    "    \n",
    "    # BUILDING ADJACENCY MATRIX\n",
    "      \n",
    "    # 1. Recuperar todos los items (nodes) de la base de datos\n",
    "    nodes = items\n",
    " \n",
    "    # 2. I create a dictionary: keys are the items, and values are the list of users that are interacted with this item\n",
    "    grouped = df_interactions.groupby('item')['user'].apply(list)\n",
    "\n",
    "    links = list()\n",
    "    \n",
    "    nodes_with_interaction = df_interactions.item.unique()\n",
    "\n",
    "    # 4. create the links with the suitable format for nx\n",
    "    links = createLinks(grouped, nodes_with_interaction, WEIGHT_THRESHOLD)\n",
    " \n",
    "    # 5. I create the graph\n",
    "    graph = create_graph_nx(nodes, links)\n",
    "\n",
    "    adjacency_matrix = build_adjacency_matrix(graph, list_nodes=nodes)\n",
    "   \n",
    "    # # GETTING DATA TO BUILD THE COMMUNITY MEMBERSHIP INFORMATION MATRIX AND ATTRIBUTE SIMILARITY MATRIX\n",
    "\n",
    "    column_names = [\"Actual_price_log\", \"Category_name_encoded_log\"]\n",
    "    features = df_items[column_names]\n",
    "\n",
    "    scaler = RobustScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # BUILDING COMMUNITY MEMBERSHIP INFORMATION MATRIX\n",
    "    \n",
    "    # Applying K-MEANS to build commnunity membership information matrix\n",
    "    kmeans_kwargs = {\n",
    "        \"init\": \"k-means++\",\n",
    "        \"n_init\": 10,\n",
    "        \"max_iter\": 300,\n",
    "    }\n",
    "\n",
    "    # Applying k-means clustering\n",
    "    \n",
    "    # n_clusters must be selected according to elbow_method and silhouette_score\n",
    "    n_clusters = 2 \n",
    "    kmeans_df = apply_kmeans(scaled_features, n_clusters, kmeans_kwargs,\n",
    "            x=column_names[0], y=column_names[1], feature_names=column_names)\n",
    "    community_matrix = build_community_information_matrix(cluster_assignments=kmeans_df[\"predicted_cluster\"]) \n",
    "\n",
    "    # BUILDING THE ATTRIBUTE SIMILARITY MATRIX\n",
    "    \n",
    "    # Applying RBF to build attribute similarity matrix\n",
    "    similarity_matrix = apply_rbf_to_get_similarity(features=scaled_features)\n",
    "\n",
    "    # APPLYING ACCSLP MODEL FOR ADJACENCY MATRIX PREDICTIONS       \n",
    "    nmf_accslp = NMF_ACCSLP(verbose=1)\n",
    "\n",
    "    U = nmf_accslp.fit_transform(S=adjacency_matrix, Z=similarity_matrix, X=community_matrix)\n",
    "    H = nmf_accslp.components_\n",
    "\n",
    "    S_prime = U @ H\n",
    "\n",
    "    S_prime = S_prime[user_items_train_indexes]\n",
    " \n",
    "    user_list_to_recommend = [user]\n",
    "\n",
    "    df_users = pd.DataFrame({'user_id':grouped_user.index, 'list_item_id':grouped_user.values})\n",
    "    \n",
    "    df_new = df_users[df_users['user_id'].isin(user_list_to_recommend)]\n",
    "\n",
    "    dataframe_k_measures_original = list()\n",
    "\n",
    "    dataframe_k_measures_original = [apply_getKrecommendations(df_new, user_items_train, user_items_train_indexes, S_prime, k, items, user_items_eval).copy() for k in range_K]\n",
    "    \n",
    "    metrics_results = [calculateMetricsResults(dataframe_k_measures_original[k]['list_recommendations'].tolist(), dataframe_k_measures_original[k]['list_recommendations_original'].tolist(), user_list_to_recommend, user_items_train, user_items_eval, range_K[k]) for k in range(K)]\n",
    "    \n",
    "    return metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "../src\\_nmf_ACCSLP.py:805: UserWarning: The multiplicative update ('mu') solver cannot update zeros present in the initialization, and so leads to poorer results when used jointly with init='nndsvd'. You may try init='nndsvda' or init='nndsvdar' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing  Matrix S\n",
      "Initializing  Matrix X\n",
      "Initializing  Matrix Z\n",
      "Epoch 10 reached after 0.379 seconds, error: 17295636.049920\n",
      "(previous_error - error) / error_at_init): 0.0015078607324064141\n",
      "Epoch 20 reached after 0.718 seconds, error: 17295449.831717\n",
      "(previous_error - error) / error_at_init): 1.075053909973802e-05\n",
      "Items a recomendar, con su peso:  [(1, 0.2011453683354821), (0, 0.1960755275615854), (8, 0.1721595299126907), (5, 0.17192760209963465), (2, 0.11487511218472488), (6, 0.11342409779976768), (7, 0.030392762106114316)]\n",
      "list_final [1, 0, 8, 5, 2, 6, 7]\n",
      "Items a recomendar, con su peso:  [(1, 0.2011453683354821), (0, 0.1960755275615854), (8, 0.1721595299126907), (5, 0.17192760209963465), (2, 0.11487511218472488), (6, 0.11342409779976768), (7, 0.030392762106114316)]\n",
      "list_final [1, 0, 8, 5, 2, 6, 7]\n",
      "Items a recomendar, con su peso:  [(1, 0.2011453683354821), (0, 0.1960755275615854), (8, 0.1721595299126907), (5, 0.17192760209963465), (2, 0.11487511218472488), (6, 0.11342409779976768), (7, 0.030392762106114316)]\n",
      "list_final [1, 0, 8, 5, 2, 6, 7]\n",
      "   user_id     train_items eval_items recom_items   recom_items_original\n",
      "0        2  [3, 4, 10, 11]  [0, 2, 5]   [1, 0, 8]  [1, 0, 8, 5, 2, 6, 7]\n",
      "   user_id     train_items eval_items      recom_items   recom_items_original\n",
      "0        2  [3, 4, 10, 11]  [0, 2, 5]  [1, 0, 8, 5, 2]  [1, 0, 8, 5, 2, 6, 7]\n",
      "   user_id     train_items eval_items            recom_items  \\\n",
      "0        2  [3, 4, 10, 11]  [0, 2, 5]  [1, 0, 8, 5, 2, 6, 7]   \n",
      "\n",
      "    recom_items_original  \n",
      "0  [1, 0, 8, 5, 2, 6, 7]  \n"
     ]
    }
   ],
   "source": [
    "df_items = pd.read_csv('./../datasets/reduced_electronic_products.csv')\n",
    "df_interactions = pd.read_csv('./../datasets/interactions.csv')\n",
    "\n",
    "user = 2\n",
    "user_test_interactions = df_interactions[df_interactions.user == user].sample(frac=TEST_SET_RATIO)\n",
    "df_interactions = df_interactions[~df_interactions.index.isin(user_test_interactions.index)]\n",
    "\n",
    "items = df_items['id'].tolist()\n",
    "\n",
    "user_items_eval = sorted(user_test_interactions[\"item\"].tolist())\n",
    "user_items_train = df_interactions[df_interactions.user == user][\"item\"].tolist()\n",
    "user_items_train_indexes = np.where(np.isin(items, user_items_train)) # This returns a tuple\n",
    "user_items_train_indexes = np.array(user_items_train_indexes[0])\n",
    "\n",
    "metric_results = main_process(items, user_items_train, user_items_train_indexes, df_interactions, user_items_eval, user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_results = [calculateScoreResults(metric_results[k]) for k in range(K)]\n",
    "# [write_results_file(\"./../results/results.csv\", metrics_results[k], range_K[k]) for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
