{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "# LIBRARIES\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "from datetime import datetime\n",
    "import networkx as nx\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import operator\n",
    "# Documentacion de la libreria: http://networkx.readthedocs.io/en/networkx-1.11/\n",
    "\n",
    "from operator import itemgetter\n",
    "from itertools import groupby\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONSTANTS \n",
    "\n",
    "# DATASET_SIZE = 100000\n",
    "# DATASET_SIZE =  15954 # most popular movies 200 && users que han interactuado con 50%\n",
    "# DATASET_SIZE =  34030 # most popular movies 200 && users que han interactuado con 25%\n",
    "DATASET_SIZE =  42482 # most popular movies 200 && users que han interactuado con 12.5%\n",
    "\n",
    "# HALF_DATASET_SIZE = int(90*DATASET_SIZE / 100)\n",
    "# HALF_DATASET_SIZE = int(75*DATASET_SIZE / 100)\n",
    "# SECOND_HALF_DATASET = int(DATASET_SIZE - HALF_DATASET_SIZE)\n",
    "RATING_THRESHOLD = 4\n",
    "WEIGHT_THRESHOLD = 5\n",
    "# K = 10\n",
    "K = 3\n",
    "range_K = [3,5,10]\n",
    "MEASURES = ['aa', 'cn', 'ew', 'jn', 'pa', 'waa', 'wcn', 'wpa']\n",
    "USERS_EVAL = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compareNodes(f_list, s_list):\n",
    "    \"\"\"\n",
    "        Function that returns the number of users that have interact with both items\n",
    "        Funcion que devuelve el numero de usuarios que han interactuado con ambos items\n",
    "    \"\"\"\n",
    "    peso = len(np.intersect1d(f_list, s_list))\n",
    "    \n",
    "    return peso\n",
    "    \n",
    "def createLinks(prob_us_set, nodos, threshold):\n",
    "    \"\"\"\n",
    "        Function that creates graph links with the information about the set. The weight has to be grater or equal to threshold.\n",
    "        \n",
    "        Funcion que crea los enlaces del grafo a partir de la informacion contenida en el conjunto que se le\n",
    "        pasa a la funcion. El peso tiene que ser mayor o igual al umbral.\n",
    "        \n",
    "        Format of links list -> [(Node1, Node2, weight), ......]\n",
    "    \"\"\"\n",
    "    resultado = list()  \n",
    "    \n",
    "    # hago todas las posibles combinaciones de problemas\n",
    "    for fst, snd in it.combinations(nodos, 2):\n",
    "        # obtengo el peso pasando la lista de usuarios que ha hecho cada problema\n",
    "        peso = compareNodes(prob_us_set[fst], prob_us_set[snd])\n",
    "        if peso >= threshold:\n",
    "            resultado.append((fst, snd, peso))\n",
    "            \n",
    "            \n",
    "            \n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_nx(list_nodes, list_links):\n",
    "    \"\"\"\n",
    "        Function that creates a graph with the format from NetworkX \n",
    "        \n",
    "        Funcion que crea un grafo de tipo Graph de la libreria NetworkX\n",
    "        Construccion del grafo: http://networkx.readthedocs.io/en/networkx-1.11/tutorial/tutorial.html#what-to-use-as-nodes-and-edges\n",
    "    \"\"\"\n",
    "    grafo = nx.Graph() # creo la variable grafo\n",
    "\n",
    "    # incluyo los nodos del grafo \n",
    "    grafo.add_nodes_from(list_nodes)\n",
    "\n",
    "    # se incluyen las tuplas de enlaces con el peso del enlace\n",
    "    # es una lista de la forma [(Nodo1, Nodo2, peso), ......]\n",
    "    grafo.add_weighted_edges_from(list_links)\n",
    "\n",
    "    return grafo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nodes_connected(u, v, graph):\n",
    "    return u in graph.neighbors(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def areAccessible(relevant, possible, graph):\n",
    "    access = list()\n",
    "    for r in relevant:\n",
    "        for p in possible:\n",
    "            if nodes_connected(r, p, graph):\n",
    "                access.append(r)\n",
    "    return list(set(access))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_aa(i1, i2, graph):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve para cada par de nodos, el sumatorio de 1/log(N(z)), siendo N(z) el grado del nodo z para todo z \n",
    "        perteneciente al conjunto de nodos en comun de ese par de nodos\n",
    "    \"\"\"\n",
    "    \n",
    "    # obtengo un iterador de un solo elemento que tiene en la tercera posicion el valor de AA para el par de nodos\n",
    "    value = nx.adamic_adar_index(graph, [(i1, i2)])\n",
    "    \n",
    "    value_aa = 0\n",
    "    for u, v, p in value:\n",
    "        # itero el iterador, guardando el valor de adar adamic\n",
    "        value_aa = p\n",
    "    \n",
    "    return value_aa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_cn(i1, i2, graph):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve el numero de vecinos en comun de esos dos nodos\n",
    "    \"\"\"\n",
    "    return len(list(nx.common_neighbors(graph, i1, i2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_ew(fst, snd, graph):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve el peso del enlace en cada par\n",
    "    \"\"\"\n",
    "    \n",
    "    weight = graph.get_edge_data(fst, snd)\n",
    "    \n",
    "    # print(weight)\n",
    "    \n",
    "    if weight == None: # devuelve 0 en caso de que no exista enlace\n",
    "        return 0\n",
    "    else: # si si existe, devuelve el peso\n",
    "        return weight['weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_jn(i1, i2, graph):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve el numero de vecinos en comun de esos dos nodos\n",
    "    \"\"\"\n",
    "    values_jn = nx.jaccard_coefficient(graph, [(i1, i2)])\n",
    "    \n",
    "    value_jn = 0\n",
    "    for u, v, p in values_jn:\n",
    "        value_jn = p # saco el valor\n",
    "        \n",
    "    return value_jn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_pa(i1, i2, graph):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve el valor de preferential attachment\n",
    "    \"\"\"\n",
    "    values_pa = nx.preferential_attachment(graph, [(i1, i2)])\n",
    "    \n",
    "    value_pa = 0\n",
    "    for u, v, p in values_pa:\n",
    "        value_pa = p # saco el valor\n",
    "        \n",
    "    return value_pa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_waa(i1, i2, graph):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve para cada par de nodos, el valor de weighted adar adamic\n",
    "    \"\"\"\n",
    "    \n",
    "    # primero tengo que calcular los common neighbors de ambos items\n",
    "    cn_list = nx.common_neighbors(graph, i1, i2)\n",
    "    \n",
    "    # ahora tengo que hacer el sumatorio del valor para cada elemento de cn_list\n",
    "    value_waa = sum([((graph[i1][x]['weight'] + graph[i2][x]['weight']) / math.log(1 + graph.degree(x, weight=\"weight\"), 10) )  for x in cn_list])    \n",
    "    \n",
    "    \n",
    "    return value_waa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wcn(i1, i2, graph):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve el numero de vecinos en comun de esos dos nodos\n",
    "    \"\"\"\n",
    "    cn_list = nx.common_neighbors(graph, i1, i2)\n",
    "    \n",
    "    value_wcn = sum([graph[i1][x]['weight'] + graph[i2][x]['weight'] for x in cn_list])\n",
    "    \n",
    "    return value_wcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_wpa(i1, i2, graph):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve el valor de weighted preferential attachment\n",
    "    \"\"\"\n",
    "    value_wpa = graph.degree(i1, weight=\"weight\") * graph.degree(i2, weight=\"weight\")\n",
    "        \n",
    "    return value_wpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_measure(i1, i2, graph, measure):\n",
    "     \n",
    "    \"\"\"\n",
    "        Function that resturn similarity value with the measure\n",
    "        \n",
    "    \"\"\"\n",
    "    # Aplico la funcion a cada fila\n",
    "    if measure == 'aa':\n",
    "        sim_result = apply_aa(i1, i2, graph)\n",
    "    elif measure == 'cn':\n",
    "        sim_result = apply_cn(i1, i2, graph)\n",
    "    elif measure == 'ew':\n",
    "        sim_result = apply_ew(i1, i2, graph)\n",
    "    elif measure == 'jn':    \n",
    "        sim_result = apply_jn(i1, i2, graph)\n",
    "    elif measure == 'pa':    \n",
    "        sim_result = apply_pa(i1, i2, graph)\n",
    "\n",
    "    elif measure == 'waa':\n",
    "        sim_result = apply_waa(i1, i2, graph)\n",
    "    elif measure == 'wcn':    \n",
    "        sim_result = apply_wcn(i1, i2, graph)\n",
    "    elif measure == 'wpa':    \n",
    "        sim_result = apply_wpa(i1, i2, graph)\n",
    "        \n",
    "    return sim_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_matrix_items(items, items_eval, graph, user):\n",
    "    \"\"\" en cada fila, voy a tener el conjunto de items a evaluar\n",
    "        en las columnas, todos los items del grafo\"\"\"\n",
    "    \n",
    "    if not os.path.exists('similarities/item_weigh/user_' + str(user) + '/'):\n",
    "        os.makedirs('similarities/item_weigh/user_' + str(user) + '/')\n",
    "                          \n",
    "    for measure in MEASURES:\n",
    "        with open('similarities/item_weigh/user_' + str(user) + '/item_' + measure + '.csv', 'w') as result_file:\n",
    "            print('item1,item2,similarity', file=result_file)\n",
    "\n",
    "            for i1 in items_eval:\n",
    "                for i2 in items:\n",
    "                    sim = apply_measure(i1, i2, graph, measure)\n",
    "                    print(f\"{i1},{i2},{sim}\", file=result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getting_sim_matrix(user):\n",
    "    \"\"\" Read the arrays\"\"\"\n",
    "    pd_sim_aa = pd.read_csv('similarities/item_weigh/user_' + str(user) + '/item_aa.csv')\n",
    "    pd_sim_cn = pd.read_csv('similarities/item_weigh/user_' + str(user) + '/item_cn.csv')\n",
    "    pd_sim_ew = pd.read_csv('similarities/item_weigh/user_' + str(user) + '/item_ew.csv')\n",
    "    pd_sim_jn = pd.read_csv('similarities/item_weigh/user_' + str(user) + '/item_jn.csv')\n",
    "    pd_sim_pa = pd.read_csv('similarities/item_weigh/user_' + str(user) + '/item_pa.csv')\n",
    "    pd_sim_waa = pd.read_csv('similarities/item_weigh/user_' + str(user) + '/item_waa.csv')\n",
    "    pd_sim_wcn = pd.read_csv('similarities/item_weigh/user_' + str(user) + '/item_wcn.csv')\n",
    "    pd_sim_wpa = pd.read_csv('similarities/item_weigh/user_' + str(user) + '/item_wpa.csv')\n",
    "\n",
    "\n",
    "    sim_aa = pd_sim_aa.pivot(index='item1', columns='item2', values='similarity')\n",
    "    sim_cn = pd_sim_cn.pivot(index='item1', columns='item2', values='similarity')\n",
    "    sim_ew = pd_sim_ew.pivot(index='item1', columns='item2', values='similarity')\n",
    "    sim_jn = pd_sim_jn.pivot(index='item1', columns='item2', values='similarity')\n",
    "    sim_pa = pd_sim_pa.pivot(index='item1', columns='item2', values='similarity')\n",
    "    sim_waa = pd_sim_waa.pivot(index='item1', columns='item2', values='similarity')\n",
    "    sim_wcn = pd_sim_wcn.pivot(index='item1', columns='item2', values='similarity')\n",
    "    sim_wpa = pd_sim_wpa.pivot(index='item1', columns='item2', values='similarity')\n",
    "    \n",
    "    return sim_aa, sim_cn, sim_ew, sim_jn, sim_pa, sim_waa, sim_wcn, sim_wpa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_recommendations(user, recom_list, measure):        \n",
    "    f = open('recommendations/item_weighted-vot_recoms.csv', 'a')\n",
    "    f.write(str(user) + ',' + measure + ',' + str(recom_list) + '\\n') \n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getKrecommendations(item_recommend, list_user_items, measure, items_eval):\n",
    "    \"\"\"\n",
    "        Funcion que devuelve la lista de k mejores problemas para el usuario dado teniendo en cuenta que \n",
    "        las recomendaciones no son items con los que haya interactuado el usuario\n",
    "    \"\"\"\n",
    "    \n",
    "    # Para row[item], el cual es el index de la columna en la matriz, encontrar los k más similares con los que no haya \n",
    "    # interactuado el usario todavia\n",
    "    # item_recommend = row['item']\n",
    "    \n",
    "    list_recom = dict()\n",
    "    i = 0\n",
    "    for elem in measure[item_recommend]:\n",
    "        if items_eval[i] not in list_user_items:\n",
    "            list_recom[items_eval[i]] = elem\n",
    "        i = i + 1\n",
    "    \n",
    "    return list(list_recom.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWeighing(item, items_recom_with_values):\n",
    "    weight = sum([value for (it, value) in items_recom_with_values if it == item])\n",
    "    \n",
    "    return weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delRepetitions(lista):\n",
    "    \"\"\"\n",
    "        Funcion auxiliar para evitar que salgan repeticiones en las recomendaciones. Saco la lista de posibles \n",
    "        recomendaciones con valores unicos\n",
    "    \"\"\"\n",
    "    conjunto_vacio = set()\n",
    "    \n",
    "    # esto sirve para que se haga mas rapido la comprobacion de si el elemento esta en la lista o no\n",
    "    function_add = conjunto_vacio.add\n",
    "    \n",
    "    # hago la lista intensional, para mantener el orden dado en la lista original\n",
    "    return [x for x in lista if not (x in conjunto_vacio or function_add(x))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getListRecom(df_users, k, measure, items_eval, user):\n",
    "    list_recommendations = list()\n",
    "    for item_recommend in df_users[user]:\n",
    "        # Esto va a contener una lista de tuplas del tipo (item-recomendar, valor similitud), donde el item puede aparecer varias veces repetido\n",
    "        list_recommendations = list_recommendations + getKrecommendations(item_recommend, df_users[user], measure, items_eval)\n",
    "\n",
    "    # Contiene la suma de todos los valroes de similitud\n",
    "    total = sum([x for (_,x) in list_recommendations])\n",
    "\n",
    "    # Esto va a contener solo los items a recomendar sin repetición y sin valores\n",
    "    items_recom_no_values = list(set([item for (item, _) in list_recommendations]))\n",
    "    \n",
    "    # Sistema de votación ponderada: para cada item que aparezca, sumar todos sus valores de similitud asociado / total\n",
    "    items_recom = [(item, getWeighing(item, list_recommendations)/total) for item in items_recom_no_values]\n",
    "    \n",
    "    items_recom.sort(key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # y me quedo con el primer elemento de la tupla, que es el item a recomendar\n",
    "    list_sim_final = [x for (x,_) in items_recom] \n",
    "        \n",
    "    # ahora elimino los items que estan en la lista de items con los que ha interactuado el target user\n",
    "    list_final = [x for x in list_sim_final if x not in df_users[user]]\n",
    "    \n",
    "    # y quito las repeticiones\n",
    "    list_final = delRepetitions(list_final)\n",
    "    \n",
    "    list_fin_rec = [x for x in list_final if x in items_eval]\n",
    "    \n",
    "    return list_fin_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_getKrecommendations(df_new, df_users, k, measure, items_eval, user, my_measure):\n",
    "    \"\"\"\n",
    "    Function to generate a new column with the list of recommendations for each user\n",
    "    \"\"\"\n",
    "    \n",
    "    df_new['list_recommendations_original'] = df_new.apply(lambda row: getListRecom(df_users, k, measure, items_eval, user), axis=1)\n",
    "    df_new['list_recommendations'] = df_new.apply(lambda row: row['list_recommendations_original'][:k], axis=1)    \n",
    "        \n",
    "    if k == 10:\n",
    "        df_new.apply(lambda row: write_recommendations(user, row['list_recommendations_original'], my_measure), axis=1)\n",
    "        \n",
    "        \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateMetricsResults(list_recom_items, list_recom_items_original, user_list_to_recommend, list_eval_items, list_rel_accessible, k):    \n",
    "    set_df_metric = {'user_id': user_list_to_recommend, 'eval_items': list_eval_items, 'recom_items': list_recom_items, 'rel_accessible': list_rel_accessible, 'recom_items_original': list_recom_items_original}\n",
    "    metric_df = pd.DataFrame.from_dict(set_df_metric)\n",
    "    return metric_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hit(row):\n",
    "    \"\"\"\n",
    "        Funcion que implementa la metrica one hit. Devuelve un 1 si para un usuario dado, ese usuario ha interactuado \n",
    "        con al menos uno de los items que se le ha recomendado en el evaluation_set. \n",
    "        Cero si no hay ningun item de los recomendados con los que el usuario haya interactuado\n",
    "    \"\"\"\n",
    "    num_items_common = np.intersect1d(row['recom_items'], row['eval_items'])\n",
    "    \n",
    "    if len(num_items_common) >= 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mrr(row): \n",
    "    \"\"\"\n",
    "        Funcion que va a implementar la metrica de evaluacion mrr:\n",
    "        mrr = 1/ranki, donde ranki es la posicion del primer item correcto\n",
    "    \"\"\"\n",
    "\n",
    "    num_items_common = np.intersect1d(row['recom_items'], row['eval_items'])\n",
    "    \n",
    "    if len(num_items_common) >= 1:\n",
    "\n",
    "        # hago la busqueda del primer elemento que esta en la lista de recomendados\n",
    "        fst_correct_item = -1\n",
    "        encontrado = False\n",
    "        i = 0\n",
    "        ranki = 0\n",
    "        #print(ranki)\n",
    "        while (i < len(row['recom_items'])) and (encontrado == False):\n",
    "            if row['recom_items'][i] in row['eval_items']:\n",
    "                # fst_correct_item = row['recom_items'][i]\n",
    "                # print(fst_correct_item)\n",
    "                ranki = i + 1\n",
    "                encontrado = True\n",
    "                #print(\"entro\")\n",
    "            else:\n",
    "                i = i + 1\n",
    "                \n",
    "        if ranki == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return (1/ranki)\n",
    "\n",
    "    else:\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(row):\n",
    "    \"\"\"\n",
    "        Funcion que va a implementar la metrica precision en k: \n",
    "        (cuantos de los interactuados con el usuario estan entre los recomendados) / todos los recomendados\n",
    "    \"\"\"\n",
    "    \n",
    "    num_items_common = np.intersect1d(row['recom_items'], row['eval_items'])\n",
    "    \n",
    "    # print(num_items_common)\n",
    "    \n",
    "    return (len(num_items_common)/len(row['recom_items']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(row):\n",
    "    \"\"\"\n",
    "        Funcion que implementa la metrica recall\n",
    "        (cuantos de los interactuados con el usuario estan entre los recomendados) / todos los evaluados\n",
    "    \"\"\"\n",
    "    num_items_common = np.intersect1d(row['recom_items'], row['eval_items'])\n",
    "    \n",
    "    # print(num_items_common)\n",
    "    \n",
    "    return (len(num_items_common)/len(row['eval_items']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(row):\n",
    "    \"\"\"\n",
    "        Funcion que calcula el f1 en funcion de precision y recall\n",
    "    \"\"\"\n",
    "    denominador = row['precision'] + row['recall']\n",
    "    \n",
    "    if denominador == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (2 * row['precision'] * row['recall']) / denominador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rPrecision(row):\n",
    "    \"\"\"\n",
    "        Funcion que va a implementar la metrica r-precision: cuales de los recomendados \n",
    "        son relevantes en el conjunto de accesibles por el grafo \n",
    "    \"\"\"\n",
    "    if len(row['rel_accessible']) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        recomendations = row['recom_items_original'][:len(row['rel_accessible'])]\n",
    "        num_items_common = np.intersect1d(recomendations, row['rel_accessible'])\n",
    "\n",
    "        # print(num_items_common)\n",
    "\n",
    "        return (len(num_items_common)/len(row['rel_accessible']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateScoreResults(metric_df):\n",
    "    \"\"\"\n",
    "        Function to build a dataframe with the results for the evaluation metrics\n",
    "    \"\"\"\n",
    "    metric_df['one_hit'] = metric_df.apply(lambda row: one_hit(row), axis=1)\n",
    "    metric_df['mrr'] = metric_df.apply(lambda row: mrr(row), axis=1)\n",
    "    metric_df['precision'] = metric_df.apply(lambda row: precision(row), axis=1)\n",
    "    metric_df['recall'] = metric_df.apply(lambda row: recall(row), axis=1)\n",
    "    metric_df['f1'] = metric_df.apply(lambda row: f1(row), axis=1)\n",
    "    metric_df['rprec'] = metric_df.apply(lambda row: rPrecision(row), axis=1)\n",
    "\n",
    "    result_one_hit = metric_df['one_hit'].mean()\n",
    "    result_precision = metric_df['precision'].mean()\n",
    "    result_mrr = metric_df['mrr'].mean()\n",
    "    result_recall = metric_df['recall'].mean()\n",
    "    result_f1 = metric_df['f1'].mean()\n",
    "    result_rprec = metric_df['rprec'].mean()\n",
    "\n",
    "    # voy a crear un diccionario con los resultados\n",
    "    results_metrics = {'one_hit': result_one_hit, 'precision': result_precision, 'mrr': result_mrr, 'recall': result_recall, 'f1': result_f1, 'rprec': result_rprec}\n",
    "    # results_metrics = {'rprec': result_rprec}\n",
    "\n",
    "    \n",
    "    return results_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_process(user, items_eval):\n",
    "    #Conjunto con todas las interacciones de los usuarios, fueron extraidas las interacciones del usuario\n",
    "    # objetivo con los items bajo evaluación.\n",
    "    training_set = pd.read_csv('user-datasets/user_' + str(user) + '_training.csv')\n",
    "\n",
    "    # Conjunto de interacciones del usuario objetivo con los productos seleccionados para evaluar.\n",
    "    evaluation_set = pd.read_csv('user-datasets/user_' + str(user) + '_test.csv')\n",
    "\n",
    "    # I get the list of nodes\n",
    "    nodes = training_set.item.unique()\n",
    "    print(len(nodes))\n",
    "\n",
    "    # I create a dictionary: keys are the items, and values are the list of users that are interacted with this item\n",
    "    grouped = training_set.groupby('item')['user'].apply(list)\n",
    "\n",
    "    # I create the links with the suitable format for nx\n",
    "    links = createLinks(grouped, nodes, WEIGHT_THRESHOLD)\n",
    "    print(len(links))\n",
    "\n",
    "    ##################### GRAPH CREATION\n",
    "    # I create the graph\n",
    "    graph = create_graph_nx(nodes, links)\n",
    "    \n",
    "    # diccionario que va a contener como key el user, como value, los items con los que ha interactuado el user\n",
    "    df_users_simple = {}\n",
    "    grouped_user = training_set.groupby('user')['item'].apply(list)\n",
    "    for i,j in zip(grouped_user.index.tolist(), grouped_user.values.tolist()):\n",
    "        df_users_simple[i] = j \n",
    "        \n",
    "    # PARA LA CONSTRUCCION DE R PRECISION -----------\n",
    "    grouped_user_eval = evaluation_set.groupby('user')['item'].apply(list)\n",
    "\n",
    "    # convierto la serie en un dataframe\n",
    "    df_users_eval = pd.DataFrame({'user_id':grouped_user_eval.index, 'list_item_id':grouped_user_eval.values})\n",
    "\n",
    "    user_list_to_recommend = list(evaluation_set.user.unique())\n",
    "\n",
    "    # hago el filtro para los usuarios a los que tengo que recomendar\n",
    "    df_users_eval = df_users_eval[df_users_eval['user_id'].isin(user_list_to_recommend)]\n",
    "    # primero voy a ordenar la lista de usuarios a recomendar\n",
    "    user_list_to_recommend.sort()\n",
    "    list_eval_items = df_users_eval['list_item_id'].tolist()\n",
    "\n",
    "    df_users_eval[\"films_watched\"] = df_users_eval.apply (lambda row: df_users_simple[row['user_id']], axis=1)\n",
    "    df_users_eval[\"rel_accessible\"] = df_users_eval.apply (lambda row: areAccessible(row['list_item_id'], row['films_watched'], graph), axis=1)\n",
    "    df_users_eval[\"num_accessible\"] = df_users_eval.apply (lambda row: len(row['rel_accessible']), axis=1)\n",
    "    list_rel_accessible = df_users_eval['rel_accessible'].tolist()\n",
    "    list_eval_items = df_users_eval['list_item_id'].tolist()\n",
    "    user_list_to_recommend = df_users_eval['user_id'].tolist()\n",
    "    \n",
    "    \n",
    "    # construccion de matrices de similitud\n",
    "    \"\"\"\n",
    "        items: Son todos los id de los productos que forman parte del conjunto de los 200 productos mas populares con el que el 12% de los usuarios han interactuado\n",
    "        items_eval: Son el 20% de los productos seleccionados aleatoriamente del 100% del conjunto de productos (12% de las peliculas mas importantes)\n",
    "        graph: es el grafo con las interacciones del conjunto del 12% de las peliculas mas populares.\n",
    "        user: Es el usuario que se esta evaluando\n",
    "    \"\"\"\n",
    "    build_matrix_items(items, items_eval, graph, user)\n",
    "\n",
    "    sim_aa, sim_cn, sim_ew, sim_jn, sim_pa, sim_waa, sim_wcn, sim_wpa = getting_sim_matrix(user)\n",
    "    list_measures = [sim_aa, sim_cn, sim_ew, sim_jn, sim_pa, sim_waa, sim_wcn, sim_wpa]\n",
    "\n",
    "    # convierto la serie en un dataframe\n",
    "    df_users = pd.DataFrame({'user_id':grouped_user.index, 'list_item_id':grouped_user.values})\n",
    "    \n",
    "    df_new = df_users[df_users['user_id'].isin(user_list_to_recommend)]\n",
    "#     df_new = df_users.groupby('user_id').list_item_id.apply(lambda x: pd.DataFrame(x.values[0])).reset_index().drop('level_1', axis = 1)\n",
    "#     df_new.columns = ['user_id','item']\n",
    "    \n",
    "    dataframe_k_measures_original = list()\n",
    "\n",
    "    # df_new: contiene el usuario al cual se le haran las recomendaciones, este dataframe contiene una columna con todos los productos con los que\n",
    "    # ha interactuado\n",
    "    # df_users_simple: es un diccionario que contiene todos los usuarios, cuyo value es una lista de productos con los que han interactuado\n",
    "    # k: Es el tamaño de la lista de recomendación\n",
    "    # items_eval: Es la lista del 10% de elementos que fueron seleccionados para evaluar\n",
    "    #\n",
    "    dataframe_k_measures_original = [[apply_getKrecommendations(df_new, df_users_simple, k, list_measures[MEASURES.index(measure)], items_eval, user, measure).copy() for measure in MEASURES] for k in range_K]\n",
    "    \n",
    "    # dataframe_k_measures_original Es una matrix que se organiza de la siguiente manera dataframe_k_measures_original[k][measure], donde cada posición tiene un dataframe con las recomendaciones correspondientes\n",
    "    \"\"\"\n",
    "        el metodo calculateMetricsResults recibe:\n",
    "        1. dataframe_k_measures_original[k][MEASURES.index(measure)]['list_recommendations'].tolist(): Lista de recomendaciones de tamaño k\n",
    "        2. dataframe_k_measures_original[k][MEASURES.index(measure)]['list_recommendations_original'].tolist(): Lista de recomendaciones con todos los elementos a recomendar\n",
    "        3. user_list_to_recommend: Lista de usuarios a recomendar\n",
    "        4. list_eval_items: Lista de productos de evaluación (Son el 20% de los productos extraidos del dataset original)\n",
    "        5. list_rel_accessible: La lista de productos accesibles en base a los productos con los que ha interactuado el usuario target, y los\n",
    "            productos que estan en el dataset de evaluación del usuario (Interacciones retiradas del dataset de entrenamiento). La accesibilidad\n",
    "            de un producto esta basada en si los productos son vecinos en el grafo de interacción\n",
    "        6. range_K[k]: Tamaño de K\n",
    "    \"\"\"\n",
    "    metrics_results = [[calculateMetricsResults(dataframe_k_measures_original[k][MEASURES.index(measure)]['list_recommendations'].tolist(), dataframe_k_measures_original[k][MEASURES.index(measure)]['list_recommendations_original'].tolist(), user_list_to_recommend, list_eval_items, list_rel_accessible, range_K[k]) for measure in MEASURES] for k in range(K)]\n",
    "\n",
    "    return metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results_file(dir, result, k, measure):\n",
    "    f = open(dir, 'a')\n",
    "    f.write(str(k) + ',' + measure + ',' + str(result['one_hit']) + ',' + str(result['precision']) + ',' + str(result['mrr']) + ',' + str(result['recall']) + ',' +  str(result['f1']) + ',' +  str(result['rprec']) + '\\n') \n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('most_pop_200_users_12.csv')\n",
    "\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sort_values('timestamp')\n",
    "\n",
    "print(df)\n",
    "\n",
    "users = df.user.unique()\n",
    "items = df.item.unique()\n",
    "num_items = len(items)\n",
    "num_users = len(users)\n",
    "\n",
    "print(num_items)\n",
    "print(num_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_eval = sorted([79, 154, 275, 137, 655, 692, 568, 235, 98, 284, 274, 204, 603, 203, 435, 245, 385, 472, 4])\n",
    "len(items_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users = [1, 2, 5, 6, 7, 8, 10, 11, 12, 13, 14, 15, 16, 18, 20, 21, 22, 23, 24, 25, 26, 28, 30, 32, 37, 38, 41, 42, 43, 44, 45, 48, 49, 52, 54, 56, 57, 58, 59, 60, 62, 63, 64, 65, 66, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 83, 84, 85, 87, 89, 90, 91, 92, 94, 95, 96, 97, 99, 101, 102, 103, 104, 106, 108, 109, 110, 113, 114, 115, 116, 117, 118, 119, 121, 123, 125, 128, 130, 135, 137, 138, 141, 144, 145, 148, 151, 152, 154, 157, 158, 159, 160, 161, 162, 164, 168, 174, 175, 177, 178, 180, 181, 183, 184, 185, 186, 187, 188, 189, 190, 193, 194, 195, 197, 198, 200, 201, 207, 210, 213, 214, 215, 216, 217, 218, 221, 222, 223, 224, 226, 227, 230, 232, 233, 234, 235, 236, 237, 239, 243, 244, 246, 248, 249, 250, 251, 253, 254, 255, 256, 259, 262, 263, 264, 265, 267, 268, 269, 270, 271, 272, 274, 275, 276, 277, 279, 280, 283, 286, 287, 288, 290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 301, 303, 305, 307, 308, 311, 312, 313, 314, 315, 316, 318, 320, 321, 322, 323, 324, 325, 326, 327, 328, 329, 330, 332, 334, 336, 338, 339, 340, 342, 343, 344, 345, 346, 347, 348, 350, 352, 354, 357, 360, 361, 363, 365, 370, 371, 372, 373, 374, 378, 379, 380, 381, 382, 383, 385, 387, 388, 389, 391, 392, 393, 394, 395, 396, 397, 398, 399, 401, 402, 403, 405, 406, 407, 409, 411, 412, 413, 416, 417, 421, 422, 423, 424, 425, 426, 429, 430, 432, 433, 435, 436, 437, 442, 445, 447, 449, 450, 452, 453, 454, 455, 456, 457, 458, 459, 460, 463, 464, 465, 466, 468, 470, 472, 474, 476, 478, 479, 480, 481, 483, 484, 486, 487, 488, 489, 490, 492, 493, 494, 495, 496, 497, 498, 499, 500, 501, 503, 504, 505, 506, 507, 508, 514, 518, 521, 523, 524, 525, 526, 527, 528, 530, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 545, 548, 551, 552, 553, 554, 555, 556, 559, 560, 561, 562, 566, 567, 568, 569, 573, 576, 577, 579, 582, 586, 587, 588, 590, 591, 592, 593, 595, 600, 601, 603, 605, 606, 608, 610, 615, 617, 618, 619, 620, 621, 622, 623, 624, 625, 627, 629, 630, 632, 633, 634, 637, 638, 639, 640, 642, 643, 645, 647, 648, 650, 653, 654, 655, 658, 659, 660, 661, 663, 664, 665, 666, 667, 668, 669, 670, 671, 674, 676, 677, 679, 680, 682, 683, 684, 686, 690, 692, 693, 694, 697, 698, 699, 703, 704, 705, 707, 708, 709, 710, 711, 712, 714, 715, 716, 717, 719, 721, 724, 727, 731, 733, 734, 735, 738, 739, 741, 745, 746, 747, 748, 749, 751, 753, 756, 757, 758, 759, 761, 763, 764, 766, 768, 770, 771, 773, 774, 776, 777, 778, 779, 780, 781, 782, 786, 788, 790, 793, 795, 796, 798, 802, 804, 805, 806, 807, 815, 821, 823, 825, 826, 829, 830, 831, 833, 834, 835, 836, 837, 838, 839, 840, 843, 844, 846, 847, 848, 850, 851, 852, 854, 860, 862, 864, 865, 867, 868, 870, 871, 872, 875, 877, 878, 880, 881, 882, 883, 885, 886, 887, 889, 890, 892, 893, 894, 896, 897, 899, 901, 902, 903, 905, 907, 908, 910, 911, 912, 913, 916, 918, 919, 921, 922, 923, 924, 927, 929, 930, 931, 932, 933, 934, 935, 936, 938, 939, 940, 942, 943]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = list()\n",
    "i = 1\n",
    "for user in users:\n",
    "    print(\"Current user: \" + str(i) + \"---\" + str(user) + \"------\")\n",
    "    results.append(main_process(user, items_eval))\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results_ev = list()\n",
    "for k in range(K):\n",
    "    list_k = list()\n",
    "    for measure in range(len(MEASURES)):\n",
    "        # Esta iterando en results para obtener el dataframe de recomendaciones para \"k\" para\n",
    "        # cada uno de los usuarios. cada elemento de results es una matriz con la forma results[k][measure], donde cada posición es un dataframe\n",
    "        # con las recomendaciones.\n",
    "        my_pd_metric = [df[k][measure] for df in results] \n",
    "        list_k.append(pd.concat(my_pd_metric))\n",
    "    metrics_results_ev.append(list_k)\n",
    "\n",
    "    # La estructura de my_pd_metric es [df1, df2], donde cada dataframe corresponde a cada usario, y para cada k y measure especifico\n",
    "    # La estructura de list_k corresponde a [my_pd_metric_1=concat([df1, df2]), my_pd_metric_2=concat([df1, df2])] \n",
    "    # donde cada sub lista corresponde a una measure distinto\n",
    "    # metrics_results_ev tiene la siguiente estructura: [[list_k_1, list_k_2], [list_k_1, list_k_2]] donde cada sub lista corresponde a un \"k\"\n",
    "    # especifico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_results = [[calculateScoreResults(metrics_results_ev[k][MEASURES.index(measure)]) for measure in MEASURES] for k in range(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[write_results_file(\"results/item_weighted-vot_12.csv\", metrics_results[k][MEASURES.index(measure)], range_K[k], measure) for measure in MEASURES] for k in range(K)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
